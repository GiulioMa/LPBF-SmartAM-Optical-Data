{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "240ee5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, SpectralClustering, DBSCAN, MeanShift, Birch\n",
    "from scipy.stats import skew\n",
    "from utils import computeBoundaries, plot_and_segment_cube_signals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3601c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_feat(material, cube_numbers, base_path):\n",
    "    \n",
    "    # Initialize the dictionary to hold the segmented data tensors\n",
    "    segmented_data_dict = {}\n",
    "\n",
    "    for cube_number in cube_numbers:\n",
    "        print(f'Working on cube {cube_number}')\n",
    "        plot_and_segment_cube_signals(base_path, params_dict, cube_number, segmented_data_dict, threshold=0.1, plot_signals=False)\n",
    "        print('---' * 10)\n",
    "\n",
    "    def extract_features(data):\n",
    "        features = {}\n",
    "\n",
    "        start_index = int(data.shape[0] * 0.0)\n",
    "        end_index = int(data.shape[0] * 1.0)\n",
    "\n",
    "        data_emission = np.mean(data[start_index:end_index, 0, :], axis=0)\n",
    "        features['mean_emission'] = np.mean(data_emission, axis=0)\n",
    "        features['std_emission'] = np.std(data_emission, axis=0)\n",
    "        features['median_emission'] = np.median(data_emission, axis=0)\n",
    "        features['95th_percentile_emission'] = np.percentile(data_emission, 95, axis=0)\n",
    "        features['5th_percentile_emission'] = np.percentile(data_emission, 5, axis=0)\n",
    "        features['skewness_emission'] = skew(data_emission, axis=0)\n",
    "\n",
    "        data_reflection = np.mean(data[start_index:end_index, 1, :], axis=0)\n",
    "        features['mean_reflection'] = np.mean(data_reflection, axis=0)\n",
    "        features['std_reflection'] = np.std(data_reflection, axis=0)\n",
    "        features['median_reflection'] = np.median(data_reflection, axis=0)\n",
    "        features['95th_percentile_reflection'] = np.percentile(data_reflection, 95, axis=0)\n",
    "        features['5th_percentile_reflection'] = np.percentile(data_reflection, 5, axis=0)\n",
    "        features['skewness_reflection'] = skew(data_reflection, axis=0)\n",
    "\n",
    "        return features\n",
    "\n",
    "    features_list = []\n",
    "    label_list = []\n",
    "    powers = []\n",
    "    speeds = []\n",
    "\n",
    "    for key, value in segmented_data_dict.items():\n",
    "        power, speed, _ = key\n",
    "        extracted_features = extract_features(value)\n",
    "        feats = [v for k, v in extracted_features.items()]\n",
    "\n",
    "        features_list.append(np.reshape(feats, (1, len(feats))))\n",
    "        label_list.append((power, speed))\n",
    "        powers.append(power)\n",
    "        speeds.append(speed)\n",
    "\n",
    "    all_data = []\n",
    "    for single_power in set(powers):\n",
    "        single_power_data = { (speed, power): val for (power, speed), val in zip(label_list, features_list) if power == single_power }\n",
    "        if single_power_data:\n",
    "            all_data.append(single_power_data)\n",
    "\n",
    "\n",
    "    def prepare_data_for_clustering(data, max_speeds, max_powers):\n",
    "        to_be_clustered = []\n",
    "        for k, v in data.items():\n",
    "            speed, power = k\n",
    "            num_rows = v.shape[0]\n",
    "\n",
    "            speed_column = np.full((num_rows, 1), speed / max_speeds)\n",
    "            power_column = np.full((num_rows, 1), power / max_powers)\n",
    "\n",
    "            extended_v = np.hstack((v, speed_column, power_column))\n",
    "            to_be_clustered.append(extended_v)\n",
    "\n",
    "        X = np.vstack(to_be_clustered)\n",
    "        X = X[X[:, -2].argsort()]\n",
    "\n",
    "        return X\n",
    "\n",
    "\n",
    "    max_speeds = max([max(k[0] for k in data.keys()) for data in all_data])\n",
    "    max_powers = max([max(k[1] for k in data.keys()) for data in all_data])\n",
    "\n",
    "\n",
    "    data_dict = {}\n",
    "    for i in range(len(all_data)):\n",
    "        data = all_data[i]\n",
    "        for k, v in data.items():\n",
    "            speed, power = k\n",
    "        X = prepare_data_for_clustering(data, max_speeds, max_powers)\n",
    "        data_dict[power] = X\n",
    "        \n",
    "    import os\n",
    "    import pickle\n",
    "        \n",
    "    def load_dictionary(filename):\n",
    "        with open(filename, 'rb') as file:\n",
    "            dictionary = pickle.load(file)\n",
    "        return dictionary\n",
    "\n",
    "    GT = load_dictionary('./GT')\n",
    "    \n",
    "    GT_dict = extract_info_by_material_and_cubes(material, cube_numbers, GT)\n",
    "    \n",
    "    # Map modes to binary labels\n",
    "    mode_mapping = {'T': 1, 'TK': 1, 'K': 1, 'C': 0}\n",
    "    gt_labels = [mode_mapping[mode] for mode in GT_dict['Modes']]\n",
    "    gt_speeds = GT_dict['Speeds']\n",
    "    gt_powers = GT_dict['Power']\n",
    "    \n",
    "    local_GT = {}\n",
    "    local_GT['Labels'] = gt_labels\n",
    "    local_GT['Speeds'] = gt_speeds\n",
    "    local_GT['Powers'] = gt_powers\n",
    "  \n",
    "    data_dict['GT'] = local_GT\n",
    "\n",
    "\n",
    "    file_name = material+'_feat.pkl'\n",
    "    file_path = os.path.join(base_path, file_name)\n",
    "\n",
    "\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(data_dict, file)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1fe84cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_path='./experiment_parameters.xlsx'\n",
    "\n",
    "# Load parameters from Excel into a dictionary\n",
    "xl = pd.ExcelFile(excel_path)\n",
    "params_dict = {sheet_name: xl.parse(sheet_name)[['Speed (mm/s)', 'Power (W)', 'Power perc (%)']].to_dict('records') for sheet_name in xl.sheet_names}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25c2cfe0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on cube 3\n",
      "Condition 1: Found 10 segments, Avg. Size: 1282.50, Std. Dev: 9.27\n",
      "Segment Max Size: 1288, Segment Min Size: 1255\n",
      "Condition 2: Found 10 segments, Avg. Size: 1536.60, Std. Dev: 2.06\n",
      "Segment Max Size: 1539, Segment Min Size: 1532\n",
      "Condition 3: Found 10 segments, Avg. Size: 1899.90, Std. Dev: 1.97\n",
      "Segment Max Size: 1905, Segment Min Size: 1897\n",
      "Condition 4: Found 10 segments, Avg. Size: 6664.00, Std. Dev: 1.26\n",
      "Segment Max Size: 6667, Segment Min Size: 6663\n",
      "Condition 5: Found 10 segments, Avg. Size: 3632.30, Std. Dev: 0.78\n",
      "Segment Max Size: 3634, Segment Min Size: 3631\n",
      "Condition 6: Found 10 segments, Avg. Size: 1903.00, Std. Dev: 1.34\n",
      "Segment Max Size: 1905, Segment Min Size: 1901\n",
      "Condition 7: Found 10 segments, Avg. Size: 1109.80, Std. Dev: 0.98\n",
      "Segment Max Size: 1111, Segment Min Size: 1108\n",
      "Condition 8: Found 10 segments, Avg. Size: 6662.30, Std. Dev: 1.62\n",
      "Segment Max Size: 6666, Segment Min Size: 6659\n",
      "Condition 9: Found 10 segments, Avg. Size: 2488.60, Std. Dev: 10.09\n",
      "Segment Max Size: 2495, Segment Min Size: 2460\n",
      "Condition 10: Found 10 segments, Avg. Size: 1502.30, Std. Dev: 14.30\n",
      "Segment Max Size: 1523, Segment Min Size: 1476\n",
      "------------------------------\n",
      "Working on cube 4\n",
      "Condition 1: Found 10 segments, Avg. Size: 2472.10, Std. Dev: 23.24\n",
      "Segment Max Size: 2495, Segment Min Size: 2418\n",
      "Condition 2: Found 10 segments, Avg. Size: 2493.50, Std. Dev: 2.11\n",
      "Segment Max Size: 2496, Segment Min Size: 2489\n",
      "Condition 3: Found 10 segments, Avg. Size: 1900.60, Std. Dev: 1.36\n",
      "Segment Max Size: 1904, Segment Min Size: 1899\n",
      "Condition 4: Found 10 segments, Avg. Size: 1288.50, Std. Dev: 1.43\n",
      "Segment Max Size: 1290, Segment Min Size: 1285\n",
      "Condition 5: Found 10 segments, Avg. Size: 3632.20, Std. Dev: 0.75\n",
      "Segment Max Size: 3633, Segment Min Size: 3631\n",
      "Condition 6: Found 10 segments, Avg. Size: 6663.00, Std. Dev: 1.41\n",
      "Segment Max Size: 6665, Segment Min Size: 6661\n",
      "Condition 7: Found 10 segments, Avg. Size: 1109.80, Std. Dev: 0.87\n",
      "Segment Max Size: 1111, Segment Min Size: 1109\n",
      "Condition 8: Found 10 segments, Avg. Size: 1533.50, Std. Dev: 1.36\n",
      "Segment Max Size: 1536, Segment Min Size: 1531\n",
      "Condition 9: Found 10 segments, Avg. Size: 1285.40, Std. Dev: 2.58\n",
      "Segment Max Size: 1289, Segment Min Size: 1280\n",
      "Condition 10: Found 12 segments, Avg. Size: 2838.92, Std. Dev: 1208.96\n",
      "Segment Max Size: 3585, Segment Min Size: 72\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "material = '316L'\n",
    "cube_numbers = [3, 4]\n",
    "\n",
    "base_path = './Data/Neuchatel_'+material+'/'\n",
    "save_feat(material, cube_numbers, base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ae577d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on cube 3\n",
      "Condition 1: Found 10 segments, Avg. Size: 1284.40, Std. Dev: 8.82\n",
      "Segment Max Size: 1288, Segment Min Size: 1258\n",
      "Condition 2: Found 10 segments, Avg. Size: 1536.70, Std. Dev: 1.10\n",
      "Segment Max Size: 1538, Segment Min Size: 1535\n",
      "Condition 3: Found 10 segments, Avg. Size: 1901.20, Std. Dev: 2.23\n",
      "Segment Max Size: 1903, Segment Min Size: 1895\n",
      "Condition 4: Found 10 segments, Avg. Size: 6663.80, Std. Dev: 1.08\n",
      "Segment Max Size: 6666, Segment Min Size: 6662\n",
      "Condition 5: Found 10 segments, Avg. Size: 3634.20, Std. Dev: 0.75\n",
      "Segment Max Size: 3635, Segment Min Size: 3633\n",
      "Condition 6: Found 10 segments, Avg. Size: 1904.40, Std. Dev: 0.66\n",
      "Segment Max Size: 1905, Segment Min Size: 1903\n",
      "Condition 7: Found 10 segments, Avg. Size: 1110.80, Std. Dev: 0.87\n",
      "Segment Max Size: 1112, Segment Min Size: 1109\n",
      "Condition 8: Found 10 segments, Avg. Size: 6664.20, Std. Dev: 0.87\n",
      "Segment Max Size: 6666, Segment Min Size: 6663\n",
      "Condition 9: Found 10 segments, Avg. Size: 2497.80, Std. Dev: 1.33\n",
      "Segment Max Size: 2501, Segment Min Size: 2496\n",
      "Condition 10: Found 10 segments, Avg. Size: 1535.40, Std. Dev: 1.02\n",
      "Segment Max Size: 1537, Segment Min Size: 1534\n",
      "------------------------------\n",
      "Working on cube 4\n",
      "Condition 1: Found 10 segments, Avg. Size: 2494.10, Std. Dev: 9.08\n",
      "Segment Max Size: 2498, Segment Min Size: 2467\n",
      "Condition 2: Found 10 segments, Avg. Size: 2498.00, Std. Dev: 1.00\n",
      "Segment Max Size: 2500, Segment Min Size: 2497\n",
      "Condition 3: Found 10 segments, Avg. Size: 1904.30, Std. Dev: 0.64\n",
      "Segment Max Size: 1905, Segment Min Size: 1903\n",
      "Condition 4: Found 10 segments, Avg. Size: 1289.50, Std. Dev: 0.50\n",
      "Segment Max Size: 1290, Segment Min Size: 1289\n",
      "Condition 5: Found 10 segments, Avg. Size: 3633.90, Std. Dev: 1.51\n",
      "Segment Max Size: 3636, Segment Min Size: 3631\n",
      "Condition 6: Found 10 segments, Avg. Size: 6664.00, Std. Dev: 1.41\n",
      "Segment Max Size: 6667, Segment Min Size: 6662\n",
      "Condition 7: Found 10 segments, Avg. Size: 1111.40, Std. Dev: 0.66\n",
      "Segment Max Size: 1113, Segment Min Size: 1111\n",
      "Condition 8: Found 10 segments, Avg. Size: 1537.50, Std. Dev: 0.67\n",
      "Segment Max Size: 1539, Segment Min Size: 1537\n",
      "Condition 9: Found 10 segments, Avg. Size: 1290.10, Std. Dev: 1.04\n",
      "Segment Max Size: 1292, Segment Min Size: 1288\n",
      "Condition 10: Found 10 segments, Avg. Size: 3633.60, Std. Dev: 0.80\n",
      "Segment Max Size: 3635, Segment Min Size: 3633\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "material = 'Ti64'\n",
    "cube_numbers = [3, 4]\n",
    "\n",
    "base_path = './Data/Neuchatel_'+material+'/'\n",
    "save_feat(material, cube_numbers, base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e4296be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Labels': [0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1],\n",
       " 'Speeds': [1550,\n",
       "  1300,\n",
       "  1050,\n",
       "  300,\n",
       "  550,\n",
       "  1050,\n",
       "  1800,\n",
       "  300,\n",
       "  800,\n",
       "  1300,\n",
       "  800,\n",
       "  800,\n",
       "  1050,\n",
       "  1550,\n",
       "  550,\n",
       "  300,\n",
       "  1800,\n",
       "  1300,\n",
       "  1550,\n",
       "  550],\n",
       " 'Powers': [90,\n",
       "  120,\n",
       "  90,\n",
       "  120,\n",
       "  120,\n",
       "  120,\n",
       "  105,\n",
       "  105,\n",
       "  90,\n",
       "  90,\n",
       "  105,\n",
       "  120,\n",
       "  105,\n",
       "  105,\n",
       "  105,\n",
       "  90,\n",
       "  120,\n",
       "  105,\n",
       "  120,\n",
       "  90]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf2643d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[90,\n",
       " 120,\n",
       " 90,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 105,\n",
       " 105,\n",
       " 90,\n",
       " 90,\n",
       " 105,\n",
       " 120,\n",
       " 105,\n",
       " 105,\n",
       " 105,\n",
       " 90,\n",
       " 120,\n",
       " 105,\n",
       " 120,\n",
       " 90]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_powers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5761e3e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
