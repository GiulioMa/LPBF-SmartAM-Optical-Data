{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "240ee5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, SpectralClustering, DBSCAN, MeanShift, Birch\n",
    "from scipy.stats import skew\n",
    "from utils import computeBoundaries, plot_and_segment_cube_signals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3601c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info_by_material_and_cubes(material, cubes, data_dict):\n",
    "    \"\"\"\n",
    "    Extract information by material and specified cubes.\n",
    "\n",
    "    Parameters:\n",
    "    - material (str): The material to filter by.\n",
    "    - cubes (list of int): The cube numbers to filter by.\n",
    "    - data_dict (dict): The dictionary containing the data.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary with keys 'Modes', 'Speeds', 'Power', containing lists of modes, speeds, and powers, respectively.\n",
    "    \"\"\"\n",
    "    extracted_info = {'Modes': [], 'Speeds': [], 'Power': []}\n",
    "    \n",
    "    if material == '316L':\n",
    "        material = 'SS'\n",
    "    if material== 'Ti64':\n",
    "        material = 'Ti'\n",
    "\n",
    "    # Filter the entries for the specified material and cubes\n",
    "    for entry in data_dict[material]:\n",
    "        if entry['Cube'] in cubes:\n",
    "            extracted_info['Modes'].append(entry['Mode'])\n",
    "            extracted_info['Speeds'].append(entry['Speed'])\n",
    "            extracted_info['Power'].append(entry['Power'])\n",
    "\n",
    "    return extracted_info\n",
    "\n",
    "def save_feat(material, cube_numbers, base_path):\n",
    "    \n",
    "    # Initialize the dictionary to hold the segmented data tensors\n",
    "    segmented_data_dict = {}\n",
    "\n",
    "    for cube_number in cube_numbers:\n",
    "        print(f'Working on cube {cube_number}')\n",
    "        plot_and_segment_cube_signals(base_path, params_dict, cube_number, segmented_data_dict, threshold=0.1, plot_signals=False)\n",
    "        print('---' * 10)\n",
    "\n",
    "    def extract_features(data):\n",
    "        features = {}\n",
    "\n",
    "        start_index = int(data.shape[0] * 0.0)\n",
    "        end_index = int(data.shape[0] * 1.0)\n",
    "\n",
    "        data_emission = np.mean(data[start_index:end_index, 0, :], axis=0)\n",
    "        features['mean_emission'] = np.mean(data_emission, axis=0)\n",
    "        features['std_emission'] = np.std(data_emission, axis=0)\n",
    "        features['median_emission'] = np.median(data_emission, axis=0)\n",
    "        features['95th_percentile_emission'] = np.percentile(data_emission, 95, axis=0)\n",
    "        features['5th_percentile_emission'] = np.percentile(data_emission, 5, axis=0)\n",
    "        features['skewness_emission'] = skew(data_emission, axis=0)\n",
    "\n",
    "        data_reflection = np.mean(data[start_index:end_index, 1, :], axis=0)\n",
    "        features['mean_reflection'] = np.mean(data_reflection, axis=0)\n",
    "        features['std_reflection'] = np.std(data_reflection, axis=0)\n",
    "        features['median_reflection'] = np.median(data_reflection, axis=0)\n",
    "        features['95th_percentile_reflection'] = np.percentile(data_reflection, 95, axis=0)\n",
    "        features['5th_percentile_reflection'] = np.percentile(data_reflection, 5, axis=0)\n",
    "        features['skewness_reflection'] = skew(data_reflection, axis=0)\n",
    "\n",
    "        return features\n",
    "\n",
    "    features_list = []\n",
    "    label_list = []\n",
    "    powers = []\n",
    "    speeds = []\n",
    "\n",
    "    for key, value in segmented_data_dict.items():\n",
    "        power, speed, _ = key\n",
    "        extracted_features = extract_features(value)\n",
    "        feats = [v for k, v in extracted_features.items()]\n",
    "\n",
    "        features_list.append(np.reshape(feats, (1, len(feats))))\n",
    "        label_list.append((power, speed))\n",
    "        powers.append(power)\n",
    "        speeds.append(speed)\n",
    "\n",
    "    all_data = []\n",
    "    for single_power in set(powers):\n",
    "        single_power_data = { (speed, power): val for (power, speed), val in zip(label_list, features_list) if power == single_power }\n",
    "        if single_power_data:\n",
    "            all_data.append(single_power_data)\n",
    "\n",
    "\n",
    "    def prepare_data_for_clustering(data, max_speeds, max_powers):\n",
    "        to_be_clustered = []\n",
    "        for k, v in data.items():\n",
    "            speed, power = k\n",
    "            num_rows = v.shape[0]\n",
    "\n",
    "            speed_column = np.full((num_rows, 1), speed / max_speeds)\n",
    "            power_column = np.full((num_rows, 1), power / max_powers)\n",
    "\n",
    "            extended_v = np.hstack((v, speed_column, power_column))\n",
    "            to_be_clustered.append(extended_v)\n",
    "\n",
    "        X = np.vstack(to_be_clustered)\n",
    "        X = X[X[:, -2].argsort()]\n",
    "\n",
    "        return X\n",
    "\n",
    "\n",
    "    max_speeds = max([max(k[0] for k in data.keys()) for data in all_data])\n",
    "    max_powers = max([max(k[1] for k in data.keys()) for data in all_data])\n",
    "\n",
    "\n",
    "    data_dict = {}\n",
    "    for i in range(len(all_data)):\n",
    "        data = all_data[i]\n",
    "        for k, v in data.items():\n",
    "            speed, power = k\n",
    "        X = prepare_data_for_clustering(data, max_speeds, max_powers)\n",
    "        data_dict[power] = X\n",
    "        \n",
    "    import os\n",
    "    import pickle\n",
    "        \n",
    "    def load_dictionary(filename):\n",
    "        with open(filename, 'rb') as file:\n",
    "            dictionary = pickle.load(file)\n",
    "        return dictionary\n",
    "\n",
    "    GT = load_dictionary('./GT')\n",
    "    \n",
    "    GT_dict = extract_info_by_material_and_cubes(material, cube_numbers, GT)\n",
    "    \n",
    "    # Map modes to binary labels\n",
    "    mode_mapping = {'T': 1, 'TK': 1, 'CT':1, 'K': 1, 'C': 0}\n",
    "    gt_labels = [mode_mapping[mode] for mode in GT_dict['Modes']]\n",
    "    gt_speeds = GT_dict['Speeds']\n",
    "    gt_powers = GT_dict['Power']\n",
    "    \n",
    "    local_GT = {}\n",
    "    local_GT['Labels'] = gt_labels\n",
    "    local_GT['Speeds'] = gt_speeds\n",
    "    local_GT['Powers'] = gt_powers\n",
    "  \n",
    "    data_dict['GT'] = local_GT\n",
    "    \n",
    "    cube_numbers_str = '_'.join(map(str, cube_numbers))\n",
    "\n",
    "    file_name = f\"{material}_cubes{cube_numbers_str}_feat.pkl\"\n",
    "    file_path = os.path.join(base_path, file_name)\n",
    "\n",
    "\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(data_dict, file)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fe84cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_path='./experiment_parameters.xlsx'\n",
    "\n",
    "# Load parameters from Excel into a dictionary\n",
    "xl = pd.ExcelFile(excel_path)\n",
    "params_dict = {sheet_name: xl.parse(sheet_name)[['Speed (mm/s)', 'Power (W)', 'Power perc (%)']].to_dict('records') for sheet_name in xl.sheet_names}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25c2cfe0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on cube 5\n",
      "Condition 1: Found 10 segments, Avg. Size: 1518.40, Std. Dev: 14.43\n",
      "Segment Max Size: 1532, Segment Min Size: 1478\n",
      "Condition 2: Found 10 segments, Avg. Size: 1284.70, Std. Dev: 1.79\n",
      "Segment Max Size: 1288, Segment Min Size: 1282\n",
      "Condition 3: Found 10 segments, Avg. Size: 1109.60, Std. Dev: 0.92\n",
      "Segment Max Size: 1111, Segment Min Size: 1108\n",
      "Condition 4: Found 10 segments, Avg. Size: 6662.40, Std. Dev: 0.80\n",
      "Segment Max Size: 6663, Segment Min Size: 6661\n",
      "Condition 5: Found 11 segments, Avg. Size: 1259.45, Std. Dev: 92.18\n",
      "Segment Max Size: 1291, Segment Min Size: 968\n",
      "Condition 6: Found 11 segments, Avg. Size: 1289.36, Std. Dev: 1.49\n",
      "Segment Max Size: 1292, Segment Min Size: 1286\n",
      "Condition 7: Found 10 segments, Avg. Size: 1900.40, Std. Dev: 0.66\n",
      "Segment Max Size: 1902, Segment Min Size: 1900\n",
      "Condition 8: Found 10 segments, Avg. Size: 2494.80, Std. Dev: 1.40\n",
      "Segment Max Size: 2496, Segment Min Size: 2491\n",
      "Condition 9: Found 10 segments, Avg. Size: 1534.80, Std. Dev: 1.54\n",
      "Segment Max Size: 1537, Segment Min Size: 1532\n",
      "Condition 10: Found 10 segments, Avg. Size: 2461.60, Std. Dev: 33.51\n",
      "Segment Max Size: 2495, Segment Min Size: 2374\n",
      "------------------------------\n",
      "Working on cube 6\n",
      "Condition 1: Found 10 segments, Avg. Size: 1529.70, Std. Dev: 10.60\n",
      "Segment Max Size: 1535, Segment Min Size: 1499\n",
      "Condition 2: Found 10 segments, Avg. Size: 3627.10, Std. Dev: 7.22\n",
      "Segment Max Size: 3632, Segment Min Size: 3608\n",
      "Condition 3: Found 10 segments, Avg. Size: 6661.80, Std. Dev: 2.96\n",
      "Segment Max Size: 6666, Segment Min Size: 6656\n",
      "Condition 4: Found 10 segments, Avg. Size: 2497.30, Std. Dev: 1.62\n",
      "Segment Max Size: 2500, Segment Min Size: 2495\n",
      "Condition 5: Found 10 segments, Avg. Size: 1900.40, Std. Dev: 0.66\n",
      "Segment Max Size: 1902, Segment Min Size: 1900\n",
      "Condition 6: Found 10 segments, Avg. Size: 3631.40, Std. Dev: 1.02\n",
      "Segment Max Size: 3633, Segment Min Size: 3629\n",
      "Condition 7: Found 10 segments, Avg. Size: 1110.20, Std. Dev: 0.87\n",
      "Segment Max Size: 1112, Segment Min Size: 1109\n",
      "Condition 8: Found 10 segments, Avg. Size: 6660.00, Std. Dev: 2.83\n",
      "Segment Max Size: 6664, Segment Min Size: 6655\n",
      "Condition 9: Found 10 segments, Avg. Size: 3611.70, Std. Dev: 34.16\n",
      "Segment Max Size: 3632, Segment Min Size: 3536\n",
      "Condition 10: Found 10 segments, Avg. Size: 1886.00, Std. Dev: 15.22\n",
      "Segment Max Size: 1901, Segment Min Size: 1858\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "material = '316L'\n",
    "cube_numbers = [5, 6]\n",
    "\n",
    "base_path = './Data/Neuchatel_'+material+'/'\n",
    "save_feat(material, cube_numbers, base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ae577d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on cube 1\n",
      "Condition 1: Found 7 segments, Avg. Size: 2196.57, Std. Dev: 12.53\n",
      "Segment Max Size: 2204, Segment Min Size: 2166\n",
      "Condition 2: Found 8 segments, Avg. Size: 2799.38, Std. Dev: 1.58\n",
      "Segment Max Size: 2802, Segment Min Size: 2797\n",
      "Condition 3: Found 10 segments, Avg. Size: 1363.40, Std. Dev: 134.26\n",
      "Segment Max Size: 1414, Segment Min Size: 962\n",
      "Condition 4: Found 6 segments, Avg. Size: 1838.00, Std. Dev: 1.83\n",
      "Segment Max Size: 1840, Segment Min Size: 1835\n",
      "Condition 5: Found 10 segments, Avg. Size: 3634.70, Std. Dev: 1.10\n",
      "Segment Max Size: 3636, Segment Min Size: 3633\n",
      "Condition 6: Found 10 segments, Avg. Size: 2499.20, Std. Dev: 0.98\n",
      "Segment Max Size: 2501, Segment Min Size: 2498\n",
      "Condition 7: Found 10 segments, Avg. Size: 1537.30, Std. Dev: 0.78\n",
      "Segment Max Size: 1539, Segment Min Size: 1536\n",
      "Condition 8: Found 10 segments, Avg. Size: 1903.10, Std. Dev: 0.83\n",
      "Segment Max Size: 1904, Segment Min Size: 1902\n",
      "Condition 9: Found 10 segments, Avg. Size: 1289.30, Std. Dev: 1.55\n",
      "Segment Max Size: 1293, Segment Min Size: 1287\n",
      "Condition 10: Found 10 segments, Avg. Size: 1110.60, Std. Dev: 0.66\n",
      "Segment Max Size: 1112, Segment Min Size: 1110\n",
      "------------------------------\n",
      "Working on cube 2\n",
      "Condition 1: Found 11 segments, Avg. Size: 2494.64, Std. Dev: 9.42\n",
      "Segment Max Size: 2499, Segment Min Size: 2465\n",
      "Condition 2: Found 10 segments, Avg. Size: 6663.40, Std. Dev: 0.92\n",
      "Segment Max Size: 6665, Segment Min Size: 6662\n",
      "Condition 3: Found 10 segments, Avg. Size: 1537.30, Std. Dev: 0.78\n",
      "Segment Max Size: 1538, Segment Min Size: 1536\n",
      "Condition 4: Found 10 segments, Avg. Size: 1903.60, Std. Dev: 0.92\n",
      "Segment Max Size: 1905, Segment Min Size: 1902\n",
      "Condition 5: Found 10 segments, Avg. Size: 6664.00, Std. Dev: 1.18\n",
      "Segment Max Size: 6666, Segment Min Size: 6662\n",
      "Condition 6: Found 10 segments, Avg. Size: 3633.50, Std. Dev: 0.81\n",
      "Segment Max Size: 3635, Segment Min Size: 3632\n",
      "Condition 7: Found 10 segments, Avg. Size: 1289.00, Std. Dev: 0.89\n",
      "Segment Max Size: 1291, Segment Min Size: 1288\n",
      "Condition 8: Found 10 segments, Avg. Size: 1289.90, Std. Dev: 0.54\n",
      "Segment Max Size: 1291, Segment Min Size: 1289\n",
      "Condition 9: Found 9 segments, Avg. Size: 6663.00, Std. Dev: 1.33\n",
      "Segment Max Size: 6666, Segment Min Size: 6661\n",
      "Condition 10: Found 11 segments, Avg. Size: 3633.36, Std. Dev: 0.88\n",
      "Segment Max Size: 3635, Segment Min Size: 3632\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "material = 'Ti64'\n",
    "cube_numbers = [1, 2]\n",
    "\n",
    "base_path = './Data/Neuchatel_'+material+'/'\n",
    "save_feat(material, cube_numbers, base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bcfc035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on cube 3\n",
      "Condition 1: Found 10 segments, Avg. Size: 1284.40, Std. Dev: 8.82\n",
      "Segment Max Size: 1288, Segment Min Size: 1258\n",
      "Condition 2: Found 10 segments, Avg. Size: 1536.70, Std. Dev: 1.10\n",
      "Segment Max Size: 1538, Segment Min Size: 1535\n",
      "Condition 3: Found 10 segments, Avg. Size: 1901.20, Std. Dev: 2.23\n",
      "Segment Max Size: 1903, Segment Min Size: 1895\n",
      "Condition 4: Found 10 segments, Avg. Size: 6663.80, Std. Dev: 1.08\n",
      "Segment Max Size: 6666, Segment Min Size: 6662\n",
      "Condition 5: Found 10 segments, Avg. Size: 3634.20, Std. Dev: 0.75\n",
      "Segment Max Size: 3635, Segment Min Size: 3633\n",
      "Condition 6: Found 10 segments, Avg. Size: 1904.40, Std. Dev: 0.66\n",
      "Segment Max Size: 1905, Segment Min Size: 1903\n",
      "Condition 7: Found 10 segments, Avg. Size: 1110.80, Std. Dev: 0.87\n",
      "Segment Max Size: 1112, Segment Min Size: 1109\n",
      "Condition 8: Found 10 segments, Avg. Size: 6664.20, Std. Dev: 0.87\n",
      "Segment Max Size: 6666, Segment Min Size: 6663\n",
      "Condition 9: Found 10 segments, Avg. Size: 2497.80, Std. Dev: 1.33\n",
      "Segment Max Size: 2501, Segment Min Size: 2496\n",
      "Condition 10: Found 10 segments, Avg. Size: 1535.40, Std. Dev: 1.02\n",
      "Segment Max Size: 1537, Segment Min Size: 1534\n",
      "------------------------------\n",
      "Working on cube 4\n",
      "Condition 1: Found 10 segments, Avg. Size: 2494.10, Std. Dev: 9.08\n",
      "Segment Max Size: 2498, Segment Min Size: 2467\n",
      "Condition 2: Found 10 segments, Avg. Size: 2498.00, Std. Dev: 1.00\n",
      "Segment Max Size: 2500, Segment Min Size: 2497\n",
      "Condition 3: Found 10 segments, Avg. Size: 1904.30, Std. Dev: 0.64\n",
      "Segment Max Size: 1905, Segment Min Size: 1903\n",
      "Condition 4: Found 10 segments, Avg. Size: 1289.50, Std. Dev: 0.50\n",
      "Segment Max Size: 1290, Segment Min Size: 1289\n",
      "Condition 5: Found 10 segments, Avg. Size: 3633.90, Std. Dev: 1.51\n",
      "Segment Max Size: 3636, Segment Min Size: 3631\n",
      "Condition 6: Found 10 segments, Avg. Size: 6664.00, Std. Dev: 1.41\n",
      "Segment Max Size: 6667, Segment Min Size: 6662\n",
      "Condition 7: Found 10 segments, Avg. Size: 1111.40, Std. Dev: 0.66\n",
      "Segment Max Size: 1113, Segment Min Size: 1111\n",
      "Condition 8: Found 10 segments, Avg. Size: 1537.50, Std. Dev: 0.67\n",
      "Segment Max Size: 1539, Segment Min Size: 1537\n",
      "Condition 9: Found 10 segments, Avg. Size: 1290.10, Std. Dev: 1.04\n",
      "Segment Max Size: 1292, Segment Min Size: 1288\n",
      "Condition 10: Found 10 segments, Avg. Size: 3633.60, Std. Dev: 0.80\n",
      "Segment Max Size: 3635, Segment Min Size: 3633\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "material = 'Ti64'\n",
    "cube_numbers = [3, 4]\n",
    "\n",
    "base_path = './Data/Neuchatel_'+material+'/'\n",
    "save_feat(material, cube_numbers, base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2643d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_powers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5761e3e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
